\chapter{Conclusion and Future Work}\label{chap:conclusion_and_future_work}

The chapter is structured in the sections "Project Summary", a discussion about the "System Limitations", and finally, "Suggestions for Future Work". 

The construction of the system is considered successful, serving as an initial milestone for future implementations and adaptations. The next steps for the advancement of this project include placing the system in a production environment, followed by the collection of user feedback. This approach will allow for an incremental learning process, in which the system will be constantly improved based on the experiences gained and the needs identified.

\section{Abstract}\label{sec:summary}
In this dissertation, a multifunctional system was developed for the collection, storage, processing, and visualization of data generated by sensors. Python was used for the creation of the \gls{API} and the processing module, MongoDB for database management, and NextJs for the construction of the control panel, known as \emph{dashboard}. The system's architecture can be seen in \ref{cap:development}.

The data were received through a multicast connection established by the data receiving module, shown in \ref{subsec:receiveDataModuleArch}. Once received, the data were immediately subjected to a preliminary analysis to identify any condition that could trigger an alert. If an alert was generated, users were notified, allowing for quick and effective interventions.

For the analysis of historical data, the BoxPlot method was employed in the processing module, explained in \ref{subsec:moduloProcessamento}. This analysis aimed at identifying patterns and anomalies in the data collected over time, providing valuable insights for the operation and maintenance of the monitored machines.

The \gls{API}, described in \ref{sec:api}, played a crucial role in the system, managing access to the data. Security was ensured through the implementation of authentication by \gls{JWT}, and several \emph{endpoints} were developed to allow effective access to the data.

The \emph{frontend}, described in \label{sec:implFront}, was responsible for displaying real-time information, generated alerts, and processed data in the form of charts.

\section{System Limitations}\label{sec:limitations}

Despite the advances achieved with the development of the system in question, some limitations were identified that could influence its effectiveness and applicability in different contexts.

Firstly, it is identified that the data receiving module needs to be specifically adapted for each operational context. This requirement may compromise the system's portability, requiring manual adjustments whenever a new application is considered.

Secondly, there are restrictions regarding the structure of the received data, as for the effective operation of the data receiving module and the alert functionality, it is necessary that the received data have an identification field and a corresponding numerical value. The absence of the latter prevents alerts from being identified and makes the processing module ineffective for the analysis of these data.

Lastly, the system was designed and tested in an environment with a limited number of connected machines. Tests were not conducted to evaluate the system's performance under the load of a large number of machines and sensors sending data simultaneously. Therefore, for larger scale scenarios, adaptations may be necessary to ensure the system's performance and effectiveness.


\section{Suggestions for Future Work}\label{sec:future_work}

Based on the observations and analyses carried out throughout this project, there are several tasks that can be performed on the system for future research and development.


\subsection{Use of artificial intelligence for prediction}

Given that the data read by the sensors are stored in the system, they have the potential to reveal significant information about the operation of the machines. The application of \gls{AI} techniques to the collected data was identified as the main functionality for the evolution of the system, which can become a robust tool for predictive maintenance. By applying machine learning algorithms to the stored data, predictive models can be generated that anticipate failures or inefficiencies in industrial equipment.

The implementation of a predictive maintenance system based on \gls{AI} could confer a significant competitive advantage to the company, not only would it improve operational efficiency, but it would also optimize the allocation of resources for maintenance, resulting in cost reduction and productivity increase. Therefore, the future exploration of \gls{AI} for the analysis of stored data is strongly recommended to enhance the effectiveness of the system under study.

\subsection{Monitoring and Logs}

Comprehensive system monitoring and activity log maintenance are crucial aspects for the system's sustainability and scalability. The absence of a well-structured log system can result in difficulties in identifying and resolving issues that may arise during the system's real-time operation. In this context, three main areas are identified where monitoring and logs could provide valuable insights for continuous improvement.

In addition, it would be advantageous to maintain a comprehensive record of data transactions between the sensors and the data receiving module. These logs could include information such as the date and time of the transaction, sensor identification, and any anomaly or failure during the receiving process. This would facilitate the verification of the integrity of the received data and assist in the early detection of possible hardware or network connectivity issues.

\subsubsection{Log for Statistical Analysis}\label{subsubsec:futurelogs}

The data processing module, responsible for statistical analysis, would also benefit from a log system. Details about the execution of the BoxPlot or any other statistical analysis could be recorded. This includes information such as the number of data points analyzed, any detected outliers, and the time taken for the analysis execution. With this information, it would be possible to further refine the analysis algorithm and identify areas for optimization.

\subsection{Dynamic Parameters for Alerts}

In the current version of the system, the parameters responsible for triggering alerts are defined statically, embedded directly in the source code. This approach, although functional, presents limitations in terms of flexibility and adaptability to different operational scenarios.

In the current configuration, any change in alert parameters requires a direct intervention in the code, followed by a testing and deployment process, which can be both time-consuming and prone to errors. In addition, the lack of flexibility limits the company's ability to quickly adapt to new operational conditions.

It would be advantageous to allow alert parameters to be configured dynamically, through the system's user interface. A feature that allows users to adjust alert thresholds and other related parameters could be implemented. The ability to make these adjustments in real time, without the need to interrupt the system's operation, would represent a significant advance in the system's usability and adaptability.

With the implementation of dynamic parameters, users could respond more quickly to changes in operational conditions, such as variations in machine workload or updates in security policies. In addition, this flexibility would increase the system's portability, facilitating its deployment in various industrial environments with distinct requirements.

\subsection{Generalization to Other Contexts}

The system developed was initially designed for a specific industrial environment. Although effective in this context, the direct transfer of the system to other industrial areas may not be trivial. Therefore, the generalization of the system to other contexts is identified as an area of interest for future work.

The data receiving module currently requires specific customization for each industrial context. In addition, the system was designed to analyze data that contains an identification field and a numerical value. The absence of these fields could hinder or make it impossible to adapt the system in environments that require the handling of different types of data.

Future research could explore methods to make the data receiving module and the processing module more flexible and adaptable to different types of data and structures. Machine learning techniques or advanced statistical methods could be applied to automate the detection of anomalous events in different scenarios, without the need for extensive manual programming.

The ability to adapt the system to different industrial contexts would not only increase its applicability, but could also lead to improvements in the efficiency of industrial operations across a variety of sectors. This is particularly relevant in a scenario where Industry 4.0 and the Internet of Things are gaining momentum \cite{nagy2018roleImpact}, and real-time data analysis is becoming increasingly critical \cite{glowalla2014processDriven} for business competitiveness.

\subsection{Performance Optimization}

The system's ability to scale and operate efficiently under high load has not been extensively tested. In particular, there are concerns about performance when a large number of machines are connected and sending data simultaneously, as well as the frontend's ability to display multiple data in real time.

The system has not yet been put into production, therefore it has also not been evaluated in an environment with high traffic, both in terms of connected machines and users accessing the dashboard. Therefore, the challenges associated with scalability, such as latency in data reception and possible bottlenecks in the database, are still unknown.

The frontend, built in Next.js, has the potential to become a bottleneck area, especially when displaying real-time data for multiple machines. The use of more recent technologies, such as Server Components in newer versions of Next.js, could contribute to more efficient rendering and better performance.

Improvements in the \gls{API} and processing module are also considered to enhance the overall efficiency of the system. Caching techniques, load balancing, and database query optimization are some of the strategies that can be explored, should performance issues arise.

To validate any implemented improvement, performance tests, high traffic simulation, and real-time monitoring are required. These tests can provide objective metrics to assess the effectiveness of optimizations and identify new areas for improvement.

System performance optimization is a priority area for future work, aiming to ensure that it can operate effectively under various load conditions, both in terms of data input and user interaction.

\subsection{Technology Updates}
Given the constantly evolving nature of software development, upgrading to newer technologies is something that cannot be neglected. In particular, newer versions of Next.js, specifically versions 13 and 14, offer features that could substantially improve the system's performance and efficiency.

One of the most promising features available in the latest versions of Next.js is the concept of Server Components \cite{nextjsServerComponents}. These components allow for more efficient rendering of user interface elements, as they are processed on the server and sent to the client as pure HTML. This reduces the load on the browser and can significantly improve the speed and efficiency of the application.

In addition, the new architecture offers more robust opportunities for caching. This is particularly useful in the context of this system, where the processing module runs at specific intervals, therefore graphics and other visual elements can be cached on the server, optimizing the user experience when accessing updated data.

It is important to note that technology updates like these require a transition period and rigorous testing to ensure that compatibility between the different elements of the system is maintained. Therefore, a well-structured migration plan and testing phases are essential for successfully implementing any update.


\subsection{Deployment and Feedback in the Factory}

A critical step for the validation and continuous improvement of the system is its deployment in a real industrial environment, preferably a factory with operations that align with the context for which the system was designed.

The deployment in a factory environment offers the opportunity to collect direct feedback from end users and stakeholders. This feedback is not only instrumental in identifying areas for immediate improvement, but also provides insights into how the system fits into the daily operations and long-term goals of the organization.

The advantage of collecting real feedback lies in the ability to make incremental adjustments to the system. These adjustments can range from correcting minor bugs to more significant modifications that can enhance the system's effectiveness. The tuning process is crucial to align the system with the needs and expectations of the users, as well as to optimize performance.


In summary, the deployment of the system in a factory environment is not an end, but rather a vital step in a cycle of continuous development and improvement. The collection of real feedback and the ability to make incremental adjustments are fundamental to ensuring that the system is efficient in a real industrial context.